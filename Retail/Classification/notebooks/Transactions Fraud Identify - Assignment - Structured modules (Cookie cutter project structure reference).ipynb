{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already raw data exists in data/raw folder. Now process the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_processed_data_script_file=os.path.join(os.path.pardir,'src','data','get_processed_data.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ..\\src\\data\\get_processed_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $get_processed_data_script_file\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def read_data():\n",
    "    '''\n",
    "    This method reads raw data and assign to Data Frame\n",
    "    '''\n",
    "    #set the path for raw data\n",
    "    raw_data_path = os.path.join(os.path.pardir, 'data','raw')\n",
    "    fraud_data_file_path = os.path.join(raw_data_path, 'fraud_data.csv')\n",
    "    fraud_ip_country_file_path = os.path.join(raw_data_path, 'IpAddress_to_Country.xlsx')\n",
    "    \n",
    "    #Read the data\n",
    "    df_fraud = pd.read_csv(fraud_data_file_path,index_col=0)\n",
    "    df_ip_country = pd.read_excel(fraud_ip_country_file_path)\n",
    "    s = pd.Series(df_ip_country['country'].values, pd.IntervalIndex.from_arrays(df_ip_country['lower_bound_ip_address'], df_ip_country['upper_bound_ip_address']))\n",
    "    df_fraud['country'] = df_fraud['ip_address'].map(s)\n",
    "    return df_fraud\n",
    "\n",
    "def processed_data(df_fraud):\n",
    "    '''\n",
    "    This method process all the data manipulations on data set including feature negineering\n",
    "    '''\n",
    "    df_fraud.country.fillna('UnKnown',inplace=True)\n",
    "    df_fraud['signup_time'] = df_fraud.signup_time.apply(pd.to_datetime)#pd.to_datetime(df_fraud.signup_time)\n",
    "    df_fraud['purchase_time'] = df_fraud.purchase_time.apply(pd.to_datetime)#pd.to_datetime(df_fraud.purchase_time)\n",
    "\n",
    "    # it is very suspicious if a user signup and then immediately purchase\n",
    "    df_fraud['time_diff'] = (df_fraud.purchase_time - df_fraud.signup_time).apply(lambda x: x.seconds)\n",
    "    \n",
    "    # Count the number of unique user ids associated each device\n",
    "    df_fraud['userids_per_ipaddress'] = df_fraud.groupby('ip_address')['user_id'].transform('count')\n",
    "\n",
    "    # Count the number of unique user ids associated each ip address\n",
    "    df_fraud['userids_per_deviceid'] = df_fraud.groupby('device_id')['user_id'].transform('count')\n",
    "    \n",
    "    # Adding age_bin transformation to df_fraud data frame\n",
    "    df_fraud['age_bin']= pd.qcut(df_fraud.age,3,labels=['Age18-30','Age30-50','Age41+'])\n",
    "\n",
    "    # Adding purchase_bin transformation to df_fraud data frame\n",
    "    df_fraud['purchase_bin']=pd.qcut(df_fraud['purchase_value'],4,labels=['low','medium','high','very_high'])\n",
    "    \n",
    "    # Add column for the average of the userids_per_deviceid,userids_per_ipaddress\n",
    "    df_fraud[\"mean_number_of_ip_device_userids\"] = (df_fraud.userids_per_deviceid + df_fraud.userids_per_ipaddress) * 0.5\n",
    "    \n",
    "    # day of the week\n",
    "    df_fraud['signup_time_dow'] = pd.to_datetime(df_fraud['signup_time']).dt.dayofweek\n",
    "    df_fraud['purchase_time_dow'] = pd.to_datetime(df_fraud['purchase_time']).dt.dayofweek\n",
    "    \n",
    "    # week of the year\n",
    "    df_fraud['signup_time_week'] = pd.to_datetime(df_fraud['signup_time']).dt.week\n",
    "    df_fraud['purchase_time_week'] = pd.to_datetime(df_fraud['purchase_time']).dt.week\n",
    "    \n",
    "    #Hour of the day\n",
    "    df_fraud['signup_hour_of_day'] = pd.to_datetime(df_fraud['purchase_time']).dt.hour\n",
    "    df_fraud['purchase_hour_of_day'] = pd.to_datetime(df_fraud['signup_time']).dt.hour\n",
    "    \n",
    "    return df_fraud\n",
    "\n",
    "\n",
    "def write_data(df):\n",
    "    processed_data_path = os.path.join(os.path.pardir,'data','processed')\n",
    "    write_fraud_data_path = os.path.join(processed_data_path,'df_fraud_data.csv')\n",
    "    df.to_csv(write_fraud_data_path)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    df=read_data()\n",
    "    df = processed_data(df)\n",
    "    write_data(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python $get_processed_data_script_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_visualized_data_script_file=os.path.join(os.path.pardir,'src','visualization','visualize.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ..\\src\\visualization\\visualize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $get_visualized_data_script_file\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import  warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "def hist_visualization(df_fraud):\n",
    "    '''\n",
    "    Histogram for distribution\n",
    "    '''\n",
    "    f, hist_fig=plt.subplots(1,2, figsize=(20,5))\n",
    "\n",
    "    sns.distplot(df_fraud['purchase_value'], hist=True, kde=True, \n",
    "                 bins=int(30), color = 'orange', \n",
    "                 hist_kws={'edgecolor':'skyblue'},\n",
    "                 kde_kws={'linewidth': 2}, ax=hist_fig[0])\n",
    "    hist_fig[0].set_title('Purchase Histogram - Right skewed 0.67')\n",
    "    hist_fig[0].set_xlabel('Bins')\n",
    "    hist_fig[0].set_ylabel('Frequency')\n",
    "\n",
    "    sns.distplot(df_fraud.age, hist=True, kde=True, \n",
    "                 bins=int(30), color = 'green', \n",
    "                 hist_kws={'edgecolor':'white'},\n",
    "                 kde_kws={'linewidth': 2}, ax=hist_fig[1])\n",
    "    hist_fig[1].set_title('Age : Histogram - Right skewed 0.42')\n",
    "    hist_fig[1].set_xlabel('Bins')\n",
    "    hist_fig[1].set_ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "def outliers_visualization(df_fraud):\n",
    "    '''\n",
    "    Out liers detection\n",
    "    '''\n",
    "    f, sub_fig=plt.subplots(1,3, figsize=(20,5))\n",
    "    sub_fig[0].scatter(df_fraud['purchase_value'],df_fraud.age,c='c', alpha=0.5)\n",
    "    sub_fig[0].set_title('Purchase vs Age')\n",
    "    sub_fig[0].set_xlabel('Purchase Value')\n",
    "    sub_fig[0].set_ylabel('Age')\n",
    "\n",
    "    sub_fig[1].boxplot(df_fraud.age)\n",
    "    sub_fig[1].set_title('Age : Box plot')\n",
    "\n",
    "    sub_fig[2].boxplot(df_fraud['purchase_value'])\n",
    "    sub_fig[2].set_title('Purchase Value : Box plot')\n",
    "\n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    "\n",
    "def other_visualization(df_fraud):\n",
    "    '''\n",
    "    Other visualizations - age, sex, class, purchase value, source, browser\n",
    "    '''\n",
    "    f, rel_fig=plt.subplots(3,3, figsize=(20,15))\n",
    "    sns.countplot(x='sex', hue='class', data=df_fraud,ax=rel_fig[0,0])\n",
    "\n",
    "    sns.catplot(x=\"class\", y=\"age\", kind=\"box\", hue='sex', data=df_fraud, ax=rel_fig[0,1]);\n",
    "\n",
    "    sns.catplot(x=\"class\", y=\"purchase_value\", kind=\"box\", hue='sex', data=df_fraud, ax=rel_fig[0,2]);\n",
    "\n",
    "    #sns.catplot(x=\"class\", y=\"purchase_value\", kind=\"box\", hue='sex', data=df_fraud, ax=rel_fig[1,0]);\n",
    "\n",
    "    sns.scatterplot(x=\"purchase_value\", y=\"age\", hue=\"class\", data=df_fraud, ax=rel_fig[1,0]);\n",
    "\n",
    "    sns.countplot(x='source', hue='class', data=df_fraud,ax=rel_fig[1,1])\n",
    "\n",
    "    sns.countplot(x='browser', hue='class', data=df_fraud,ax=rel_fig[1,2])\n",
    "\n",
    "    sns.factorplot(x=\"class\", y=\"userids_per_deviceid\", data=df_fraud, ax=rel_fig[2,0])\n",
    "\n",
    "    sns.factorplot(x=\"class\", y=\"userids_per_ipaddress\", data=df_fraud, ax=rel_fig[2,1])\n",
    "\n",
    "    h=sns.factorplot(x=\"class\", y=\"time_diff\", data=df_fraud, ax=rel_fig[2,2])\n",
    "\n",
    "    plt.close(2)\n",
    "    plt.close(3)\n",
    "    plt.close(4)\n",
    "    plt.close(5)\n",
    "    plt.close(6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def signup_purchase_visualization(df_fraud):\n",
    "    '''\n",
    "    signup hour of day, sign up week, purchase hour, purchase week\n",
    "    '''\n",
    "    f, time_fig=plt.subplots(2,3, figsize=(20,10))\n",
    "    sns.countplot(x='signup_hour_of_day', hue='class', data=df_fraud, ax=time_fig[0,0])\n",
    "    sns.countplot(x='signup_time_dow', hue='class', data=df_fraud, ax=time_fig[0,1])\n",
    "    sns.countplot(x='signup_time_week', hue='class', data=df_fraud, ax=time_fig[0,2])\n",
    "\n",
    "    sns.countplot(x='purchase_hour_of_day', hue='class', data=df_fraud, ax=time_fig[1,0])\n",
    "    sns.countplot(x='purchase_time_dow', hue='class', data=df_fraud, ax=time_fig[1,1])\n",
    "    sns.countplot(x='purchase_time_week', hue='class', data=df_fraud, ax=time_fig[1,2])\n",
    "\n",
    "    plt.close(2)\n",
    "    plt.close(3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def read_data():\n",
    "    '''\n",
    "    This method reads raw data and assign to Data Frame\n",
    "    '''\n",
    "    #set the path for raw data\n",
    "    processed_data_path = os.path.join(os.path.pardir, 'data','processed')\n",
    "    fraud_data_file_path = os.path.join(processed_data_path, 'df_fraud_data.csv')\n",
    "    \n",
    "    #Read the processed data\n",
    "    df_fraud = pd.read_csv(fraud_data_file_path,index_col=0)\n",
    "    #print(df_fraud.head(2))\n",
    "    return df_fraud\n",
    "\n",
    "if __name__=='__main__':\n",
    "    df=read_data()\n",
    "    hist_visualization(df)\n",
    "    outliers_visualization(df)\n",
    "    other_visualization(df)\n",
    "    signup_purchase_visualization(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure(2000x500)\n",
      "Figure(2000x500)\n",
      "Figure(2000x1500)\n",
      "Figure(2000x1000)\n"
     ]
    }
   ],
   "source": [
    "!python $get_visualized_data_script_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feature_engineering_data_script_file=os.path.join(os.path.pardir,'src','features','build_features.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ..\\src\\features\\build_features.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $get_feature_engineering_data_script_file\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def feature_engineering(df_fraud):\n",
    "    col_list = ['source','browser','sex','age_bin','purchase_bin','country','time_diff','mean_number_of_ip_device_userids','class']\n",
    "    df_fraud = df_fraud[col_list]\n",
    "    # Feature Engineering - create dummy variables for categorical features - onehot encoding\n",
    "    final_fraud = pd.get_dummies(df_fraud,['source','browser','sex','age_bin','purchase_bin','country'])\n",
    "    \n",
    "    return final_fraud\n",
    "\n",
    "def read_data():\n",
    "    '''\n",
    "    This method reads data and assign to Data Frame\n",
    "    '''\n",
    "    #set the path for raw data\n",
    "    processed_data_path = os.path.join(os.path.pardir, 'data','processed')\n",
    "    fraud_data_file_path = os.path.join(processed_data_path, 'df_fraud_data.csv')\n",
    "    \n",
    "    #Read the processed data\n",
    "    df_fraud = pd.read_csv(fraud_data_file_path,index_col=0)\n",
    "    #print(df_fraud.head(2))\n",
    "    return df_fraud\n",
    "\n",
    "def write_data(df):\n",
    "    processed_features_data_path = os.path.join(os.path.pardir,'data','processed')\n",
    "    write_fraud_data_feature_engineered_path = os.path.join(processed_features_data_path,'df_fraud_feature_engineered_data.csv')\n",
    "    df.to_csv(write_fraud_data_feature_engineered_path)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    df = read_data()\n",
    "    df = feature_engineering(df)\n",
    "    write_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python $get_feature_engineering_data_script_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset_data_script_file=os.path.join(os.path.pardir,'src','data','dataset_split.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ..\\src\\data\\dataset_split.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $process_dataset_data_script_file\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def process_no_target_dataset(df):\n",
    "    # Data Set with no class feature\n",
    "    X = df[ [col for col in df.columns if col != \"class\"] ]\n",
    "    return X\n",
    "\n",
    "def process_target_dataset(df):\n",
    "    # Data set with only class feature\n",
    "    Y = df[ [col for col in df.columns if col == \"class\"] ]#df[\"class\"]\n",
    "    return Y\n",
    "\n",
    "def write_data(df,csv_file):\n",
    "    processed_dataset_data_path = os.path.join(os.path.pardir,'data','processed')\n",
    "    write_fraud_data_feature_engineered_path = os.path.join(processed_dataset_data_path,csv_file)\n",
    "    df.to_csv(write_fraud_data_feature_engineered_path)\n",
    "\n",
    "def read_data():\n",
    "    '''\n",
    "    This method reads data and assign to Data Frame\n",
    "    '''\n",
    "    #set the path for raw data\n",
    "    processed_data_path = os.path.join(os.path.pardir, 'data','processed')\n",
    "    fraud_data_featured_file_path = os.path.join(processed_data_path, 'df_fraud_feature_engineered_data.csv')\n",
    "    \n",
    "    #Read the processed data\n",
    "    df_fraud = pd.read_csv(fraud_data_featured_file_path,index_col=0)\n",
    "    #print(df_fraud.head(2))\n",
    "    return df_fraud\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    df = read_data()\n",
    "    df_X = process_no_target_dataset(df)\n",
    "    df_Y = process_target_dataset(df)\n",
    "    write_data(df_X,'df_fraud_no_target_data.csv')\n",
    "    write_data(df_Y,'df_fraud_target_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python $process_dataset_data_script_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp_data_script_file=os.path.join(os.path.pardir,'src','features','feature_imp.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ..\\src\\features\\feature_imp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $feature_imp_data_script_file\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modeling imports\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix, f1_score, accuracy_score, precision_score, recall_score,precision_recall_curve \n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "import  warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "def feature_importances(X,Y):\n",
    "    '''\n",
    "    Feature importance using Extra tree classifier\n",
    "    '''\n",
    "    model = ExtraTreesClassifier()\n",
    "    model.fit(X, Y)\n",
    "    feature_imp=model.feature_importances_\n",
    "    print(feature_imp)\n",
    "\n",
    "def read_data():\n",
    "    '''\n",
    "    This method reads data and assign to Data Frame\n",
    "    '''\n",
    "    #set the path for raw data\n",
    "    processed_data_path = os.path.join(os.path.pardir, 'data','processed')\n",
    "    fraud_data_no_target_file_path = os.path.join(processed_data_path, 'df_fraud_no_target_data.csv')\n",
    "    fraud_data_target_file_path = os.path.join(processed_data_path, 'df_fraud_target_data.csv')\n",
    "    \n",
    "    #Read the processed data\n",
    "    df_fraud_x = pd.read_csv(fraud_data_no_target_file_path,index_col=0)\n",
    "    df_fraud_y = pd.read_csv(fraud_data_target_file_path, index_col=0)\n",
    "    #print(df_fraud.head(2))\n",
    "    return df_fraud_x,df_fraud_y\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    X, Y = read_data()\n",
    "    feature_importances(X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.30429516e-01 4.29584452e-01 4.65513131e-03 2.91400038e-03\n",
      " 4.18905061e-03 4.40818956e-03 5.05775245e-03 3.14822164e-03\n",
      " 2.25021219e-03 4.92235268e-03 4.63087563e-03 4.29692988e-03\n",
      " 5.89738067e-03 5.77768689e-03 5.94916464e-03 7.03874786e-03\n",
      " 6.22864143e-03 6.69758414e-03 5.83401415e-03 5.20533915e-05\n",
      " 3.26747844e-06 4.15977375e-04 4.58395112e-05 5.28781183e-07\n",
      " 8.09509672e-04 1.23360431e-04 1.22356726e-03 5.54557289e-04\n",
      " 9.43725231e-05 2.47302517e-07 1.35846039e-06 9.76843337e-05\n",
      " 3.25364533e-06 8.20330561e-05 5.09780375e-04 9.90559739e-07\n",
      " 2.46505776e-07 7.34865840e-08 2.22299589e-06 1.71425744e-04\n",
      " 8.27582620e-05 1.88091699e-06 1.54703904e-03 3.11856566e-08\n",
      " 3.07763734e-07 1.06132442e-04 1.63499174e-07 5.84922676e-06\n",
      " 2.99891885e-07 1.77682251e-03 9.11495710e-08 3.41486665e-07\n",
      " 5.37492535e-04 2.13595115e-03 7.51547029e-04 1.13682112e-06\n",
      " 4.54526117e-07 1.88627247e-04 7.24264232e-07 1.94547707e-04\n",
      " 2.01080621e-06 8.18379594e-07 4.52924636e-05 4.43932864e-04\n",
      " 5.52306623e-04 3.62344548e-07 3.98170804e-09 8.06112778e-05\n",
      " 2.19731396e-04 4.24350945e-04 6.49473896e-05 1.19630948e-05\n",
      " 6.91352045e-07 3.55329374e-04 2.86481350e-06 9.28624179e-08\n",
      " 5.34733020e-04 1.60761892e-03 1.17697156e-05 5.25408763e-07\n",
      " 1.22257209e-04 1.54906263e-03 8.64527693e-07 7.19293802e-08\n",
      " 4.49827104e-04 1.90516114e-08 8.37807395e-07 5.96871093e-05\n",
      " 8.86547244e-07 4.66198924e-05 5.08861128e-04 3.23032947e-04\n",
      " 4.42222531e-05 9.74144509e-04 5.46428293e-04 5.53140789e-04\n",
      " 3.74451248e-06 5.53478645e-04 3.50255606e-04 1.50801204e-03\n",
      " 2.68281224e-06 2.41981846e-03 1.24195441e-05 1.70935215e-04\n",
      " 1.12128971e-04 2.09553964e-03 3.13322584e-04 4.36899906e-06\n",
      " 5.46995967e-06 1.79455066e-04 2.78983524e-05 2.82255286e-07\n",
      " 3.75564488e-05 1.82146669e-07 2.50919364e-04 4.12790955e-04\n",
      " 9.87911279e-07 7.56723904e-05 1.74912322e-07 6.41421381e-05\n",
      " 2.20442198e-04 6.26553065e-07 1.31834492e-04 6.26361807e-05\n",
      " 9.58043130e-04 1.14000864e-04 8.77915090e-07 1.04395549e-05\n",
      " 6.15023491e-05 1.50021125e-04 7.08945317e-06 1.24022234e-07\n",
      " 1.20952255e-04 9.47309813e-07 9.17137238e-06 1.35637989e-03\n",
      " 4.56779787e-07 5.62055531e-04 2.89535467e-05 1.45504390e-04\n",
      " 6.69099646e-04 8.24283493e-06 2.67149183e-04 5.83372561e-06\n",
      " 1.28836261e-04 4.49084676e-07 9.45511004e-05 3.19415181e-04\n",
      " 3.03834788e-04 6.32054478e-04 4.10431973e-04 1.52221009e-05\n",
      " 1.02232464e-05 6.57140086e-07 3.73887571e-04 1.11157321e-03\n",
      " 1.05906524e-06 3.02412392e-07 7.97470069e-08 3.92386508e-04\n",
      " 4.63488795e-05 7.54053324e-05 2.13411996e-04 1.98997412e-04\n",
      " 1.63479716e-04 2.90701315e-05 8.23960612e-04 3.73069196e-09\n",
      " 8.57633572e-04 1.41202718e-04 1.21768891e-04 1.01942017e-03\n",
      " 8.13249615e-04 3.76514492e-06 1.06801327e-03 1.04038956e-06\n",
      " 3.87345476e-04 6.03083203e-05 2.18140756e-04 6.56878868e-04\n",
      " 8.11001345e-05 2.84777231e-06 5.08157838e-04 3.26491422e-03\n",
      " 1.31472146e-04 1.88003199e-03 3.80062394e-03 1.52065546e-04\n",
      " 5.54209284e-05 8.78235266e-08 3.46541464e-04 5.32600307e-04\n",
      " 7.82574275e-05 9.51406164e-09 1.20546854e-06 1.25759791e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vmuttine\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "!python $feature_imp_data_script_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_data_script_file=os.path.join(os.path.pardir,'src','data','train_test_split.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ..\\src\\data\\train_test_split.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $train_test_split_data_script_file\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modeling imports\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix, f1_score, accuracy_score, precision_score, recall_score,precision_recall_curve \n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "import  warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "def train_test_split_data(X,Y):\n",
    "    '''\n",
    "    Train Test Split\n",
    "    '''\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=1)\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def read_data():\n",
    "    '''\n",
    "    This method reads data and assign to Data Frame\n",
    "    '''\n",
    "    #set the path for raw data\n",
    "    processed_data_path = os.path.join(os.path.pardir, 'data','processed')\n",
    "    fraud_data_no_target_file_path = os.path.join(processed_data_path, 'df_fraud_no_target_data.csv')\n",
    "    fraud_data_target_file_path = os.path.join(processed_data_path, 'df_fraud_target_data.csv')\n",
    "    \n",
    "    #Read the processed data\n",
    "    df_fraud_x = pd.read_csv(fraud_data_no_target_file_path,index_col=0)\n",
    "    df_fraud_y = pd.read_csv(fraud_data_target_file_path, index_col=0)\n",
    "    #print(df_fraud.head(2))\n",
    "    return df_fraud_x,df_fraud_y\n",
    "\n",
    "def write_data(df,csv_file):\n",
    "    processed_dataset_data_path = os.path.join(os.path.pardir,'data','processed')\n",
    "    write_train_test_split_path = os.path.join(processed_dataset_data_path,csv_file)\n",
    "    df.to_csv(write_train_test_split_path)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    X, Y = read_data()\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split_data(X,Y)\n",
    "    write_data(X_train, 'X_train_data.csv')\n",
    "    write_data(X_test, 'X_test_data.csv')\n",
    "    write_data(Y_train, 'Y_train_data.csv')\n",
    "    write_data(Y_test, 'Y_test_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vmuttine\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "!python $train_test_split_data_script_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model_data_script_file=os.path.join(os.path.pardir,'src','models','build_model.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ..\\src\\models\\build_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $build_model_data_script_file\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modeling imports\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix, f1_score, accuracy_score, precision_score, recall_score,precision_recall_curve \n",
    "from sklearn.dummy import DummyClassifier\n",
    "import pickle\n",
    "\n",
    "import  warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "def read_data():\n",
    "    '''\n",
    "    This method reads data and assign to Data Frame\n",
    "    '''\n",
    "    #set the path for raw data\n",
    "    processed_data_path = os.path.join(os.path.pardir, 'data','processed')\n",
    "    x_train_file_path = os.path.join(processed_data_path, 'X_train_data.csv')\n",
    "    x_test_file_path = os.path.join(processed_data_path, 'X_test_data.csv')\n",
    "    y_train_file_path = os.path.join(processed_data_path, 'Y_train_data.csv')\n",
    "    y_test_file_path = os.path.join(processed_data_path, 'Y_test_data.csv')\n",
    "    \n",
    "    #Read the processed data\n",
    "    X_train = pd.read_csv(x_train_file_path,index_col=0)\n",
    "    X_test = pd.read_csv(x_test_file_path,index_col=0)\n",
    "    Y_train = pd.read_csv(y_train_file_path,index_col=0)\n",
    "    Y_test = pd.read_csv(y_test_file_path,index_col=0)\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def baseline_model(X_train, X_test, Y_train, Y_test):\n",
    "    model_dummy = DummyClassifier(strategy='most_frequent',random_state=0)\n",
    "    model_dummy.fit(X_train,Y_train)\n",
    "    print('score for baseline model: {0:.2f}'.format(model_dummy.score(X_test,Y_test)))\n",
    "    # Performance Metrics\n",
    "    print('accuracy for Baseline model: {0:.2f}'.format(accuracy_score(Y_test,model_dummy.predict(X_test))))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    print('Confusion matrix for Baseline model: \\n {0}'.format(confusion_matrix(Y_test,model_dummy.predict(X_test))))\n",
    "\n",
    "    # Precision and Recall scores\n",
    "    print('Precision for baseline Model: {0:.2f}'.format(precision_score(Y_test, model_dummy.predict(X_test))))\n",
    "    print('Recall for baseline Model: {0:.2f}'.format(recall_score(Y_test, model_dummy.predict(X_test))))\n",
    "\n",
    "def random_forest(X_train,X_test,Y_train,Y_test):\n",
    "    rf = RandomForestClassifier()\n",
    "    rf_model = rf.fit(X_train, Y_train)\n",
    "\n",
    "    # Predicting the results\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    #Evaluating\n",
    "    # Evaluating\n",
    "    model_test_score = rf_model.score(X_test,Y_test)\n",
    "    conf_matrix = confusion_matrix(Y_test, y_pred)\n",
    "    print ('Random Forest MODEL TEST SCORE: {0:.5f}'.format(model_test_score))\n",
    "    print(\"Random Forest ACCURACY: {0:.2f}\".format(accuracy_score(Y_test, y_pred)))\n",
    "    print(\"Random Forest ROC-AUC: {0:.2f}\".format(roc_auc_score(Y_test, y_pred)))\n",
    "    print(\"Random Forest PRECISION: {0:.2f}\".format(precision_score(Y_test, y_pred)))\n",
    "    print(\"Random Forest RECALL: {0:.2f}\".format(recall_score(Y_test, y_pred)))\n",
    "    print(\"Random Forest Confusion Matrix:\\n\",conf_matrix)\n",
    "    print ('\\nRandom Forest True Negatives: ', conf_matrix[0,0])\n",
    "    print ('Random Forest False Negatives: ', conf_matrix[1,0])\n",
    "    print ('Random Forest True Positives: ', conf_matrix[1,1])\n",
    "    print ('Random Forest False Positives: ', conf_matrix[0,1])\n",
    "    \n",
    "    return rf_model\n",
    "\n",
    "def XGBoost(X_train,X_test,Y_train,Y_test):\n",
    "    xgb = XGBClassifier()\n",
    "\n",
    "    # Fitting the model\n",
    "    xgb_model = xgb.fit(X_train, Y_train)\n",
    "\n",
    "    # Predicting results\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    #Evaluation\n",
    "    model_train_score = xgb_model.score(X_train,Y_train)\n",
    "    model_test_score = xgb_model.score(X_test,Y_test)\n",
    "    conf_matrix = confusion_matrix(Y_test, y_pred)\n",
    "    print ('XGBoost MODEL TEST SCORE: {0:.5f}'.format(model_train_score))\n",
    "    print ('XGBoost MODEL TEST SCORE: {0:.5f}'.format(model_test_score))\n",
    "    print(\"XGBoost ACCURACY: {0:.2f}\".format(accuracy_score(Y_test, y_pred)))\n",
    "    print(\"XGBoost ROC-AUC: {0:.2f}\".format(roc_auc_score(Y_test, y_pred)))\n",
    "    print(\"XGBoost PRECISION: {0:.2f}\".format(precision_score(Y_test, y_pred)))\n",
    "    print(\"XGBoost RECALL: {0:.2f}\".format(recall_score(Y_test, y_pred)))\n",
    "    print(\"XGBoost Confusion Matrix:\\n\",conf_matrix)\n",
    "    print ('\\nXGBoost True Negatives: ', conf_matrix[0,0])\n",
    "    print ('XGBoost False Negatives: ', conf_matrix[1,0])\n",
    "    print ('XGBoost True Positives: ', conf_matrix[1,1])\n",
    "    print ('XGBoost False Positives: ', conf_matrix[0,1])\n",
    "    return xgb_model\n",
    "\n",
    "def model_persistance_xgb(xgb):\n",
    "    #Create file path\n",
    "    model_file_path = os.path.join(os.path.pardir,'models','xgb_model.pkl')\n",
    "    \n",
    "    #Open file to write\n",
    "    model_file_pickle = open(model_file_path,'wb')\n",
    "    \n",
    "    #model persist\n",
    "    pickle.dump(xgb,model_file_pickle)\n",
    "    \n",
    "    model_file_pickle.close()\n",
    "    \n",
    "\n",
    "def model_persistance_rf(rf):\n",
    "    #Create file path\n",
    "    model_file_path = os.path.join(os.path.pardir,'models','rf_model.pkl')\n",
    "    \n",
    "    #Open file to write\n",
    "    model_file_pickle = open(model_file_path,'wb')\n",
    "    \n",
    "    #model persist\n",
    "    pickle.dump(rf,model_file_pickle)\n",
    "    \n",
    "    model_file_pickle.close()\n",
    "\n",
    "def validation_auc_roc(xgb_model,X_test,Y_test):\n",
    "    y_predd=xgb_model.predict_proba(X_test)\n",
    "    p = plot_validation_roc(Y_test,y_predd)\n",
    "    roc_auc = roc_auc_score(Y_test, y_predd[:,1])\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(p.FPR,p.TPR, color='orange', label='ROC curve (area = %0.2f)'%roc_auc)\n",
    "    plt.xlim([-0.02, 1])\n",
    "    plt.ylim([0, 1.02])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.plot([0, 1], [0, 1], color='skyblue', lw=2, linestyle='--',label='Random guess')\n",
    "    plt.legend(loc=\"lower right\",frameon=False)\n",
    "\n",
    "def plot_validation_roc(Y_test, y_predd):\n",
    "    fpr,tpr,thresholds = roc_curve(Y_test,y_predd[:,1])\n",
    "    return pd.DataFrame({'FPR':fpr,'TPR':tpr,'Threshold':thresholds})\n",
    "\n",
    "if __name__=='__main__':\n",
    "    X_train,X_test,Y_train,Y_test = read_data()\n",
    "    #baseline_model(X_train,X_test,Y_train,Y_test)\n",
    "    rf=random_forest(X_train,X_test,Y_train,Y_test)\n",
    "    xgb_model = XGBoost(X_train,X_test,Y_train,Y_test)\n",
    "    validation_auc_roc(xgb_model,X_test,Y_test)\n",
    "    model_persistance_rf(rf)\n",
    "    model_persistance_xgb(xgb_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest MODEL TEST SCORE: 0.94154\n",
      "Random Forest ACCURACY: 0.94\n",
      "Random Forest ROC-AUC: 0.77\n",
      "Random Forest PRECISION: 0.76\n",
      "Random Forest RECALL: 0.55\n",
      "Random Forest Confusion Matrix:\n",
      " [[21349   395]\n",
      " [ 1008  1248]]\n",
      "\n",
      "Random Forest True Negatives:  21349\n",
      "Random Forest False Negatives:  1008\n",
      "Random Forest True Positives:  1248\n",
      "Random Forest False Positives:  395\n",
      "XGBoost MODEL TEST SCORE: 0.95640\n",
      "XGBoost MODEL TEST SCORE: 0.95592\n",
      "XGBoost ACCURACY: 0.96\n",
      "XGBoost ROC-AUC: 0.77\n",
      "XGBoost PRECISION: 1.00\n",
      "XGBoost RECALL: 0.53\n",
      "XGBoost Confusion Matrix:\n",
      " [[21744     0]\n",
      " [ 1058  1198]]\n",
      "\n",
      "XGBoost True Negatives:  21744\n",
      "XGBoost False Negatives:  1058\n",
      "XGBoost True Positives:  1198\n",
      "XGBoost False Positives:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vmuttine\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "!python $build_model_data_script_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data_script_file=os.path.join(os.path.pardir,'src','models','predict_model.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ..\\src\\models\\predict_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $predict_data_script_file\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modeling imports\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix, f1_score, accuracy_score, precision_score, recall_score,precision_recall_curve \n",
    "from sklearn.dummy import DummyClassifier\n",
    "import pickle\n",
    "\n",
    "import  warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "def read_data():\n",
    "    '''\n",
    "    This method reads data and assign to Data Frame\n",
    "    '''\n",
    "    #set the path for raw data\n",
    "    processed_data_path = os.path.join(os.path.pardir, 'data','processed')\n",
    "    x_train_file_path = os.path.join(processed_data_path, 'X_train_data.csv')\n",
    "    x_test_file_path = os.path.join(processed_data_path, 'X_test_data.csv')\n",
    "    y_train_file_path = os.path.join(processed_data_path, 'Y_train_data.csv')\n",
    "    y_test_file_path = os.path.join(processed_data_path, 'Y_test_data.csv')\n",
    "    \n",
    "    #Read the processed data\n",
    "    X_train = pd.read_csv(x_train_file_path,index_col=0)\n",
    "    X_test = pd.read_csv(x_test_file_path,index_col=0)\n",
    "    Y_train = pd.read_csv(y_train_file_path,index_col=0)\n",
    "    Y_test = pd.read_csv(y_test_file_path,index_col=0)\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def predict(X_test,Y_test):\n",
    "    model_file_path = os.path.join(os.path.pardir,'models','xgb_model.pkl')\n",
    "    model_file_pickle = open(model_file_path, 'rb')\n",
    "    xgb_model = pickle.load(model_file_pickle)\n",
    "    predicted_results = xgb_model.predict(X_test)\n",
    "    return predicted_results\n",
    "\n",
    "\n",
    "def write_data(df):\n",
    "    predicted_results_data_path = os.path.join(os.path.pardir,'data','processed')\n",
    "    write_predicted_results_path = os.path.join(predicted_results_data_path,'predicted_results_X_test.csv')\n",
    "    df.to_csv(write_predicted_results_path)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    X_train,X_test,Y_train,Y_test = read_data()\n",
    "    predicted_results = predict(X_test,Y_test)\n",
    "    write_data(pd.DataFrame(predicted_results))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vmuttine\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "!python $predict_data_script_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
